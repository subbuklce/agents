{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79822182",
   "metadata": {},
   "source": [
    "# Windows MCP Server Patch Demo\n",
    "\n",
    "Demonstration of winpatch enabling MCP servers on Windows.\n",
    "\n",
    "This notebook shows both uvx and npx based MCP servers working together after applying the Windows compatibility patch.\n",
    "\n",
    "**Configuration**: Set `USE_OLLAMA = True` to use a local Ollama instance, or `False` to use OpenAI's cloud API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f744937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from agents import Agent, Runner, trace, OpenAIChatCompletionsModel\n",
    "from agents.mcp import MCPServerStdio\n",
    "from openai import AsyncOpenAI\n",
    "from winpatch import winpatch_mcpserver_stdio\n",
    "\n",
    "# Configuration: Toggle between OpenAI and Ollama\n",
    "USE_OLLAMA = False # Set to True to use Ollama, False for OpenAI\n",
    "OLLAMA_MODEL = \"qwen3:latest\"  # Ollama model to use\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"  # Ollama API base URL\n",
    "\n",
    "if USE_OLLAMA:\n",
    "    client = AsyncOpenAI(base_url=OLLAMA_BASE_URL, api_key=\"ollama\")\n",
    "    MODEL = OpenAIChatCompletionsModel(model=OLLAMA_MODEL, openai_client=client)\n",
    "else:\n",
    "    MODEL = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3779c698",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92298c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Windows compatibility patch - enables MCP servers (uvx/npx) to work on Windows\n",
    "winpatch_mcpserver_stdio()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a1a676",
   "metadata": {},
   "source": [
    "## Test 1: UVX-based MCP Server (Fetch)\n",
    "\n",
    "Testing that uvx-based MCP servers work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08881e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_params = {\"command\": \"uvx\", \"args\": [\"mcp-server-fetch\"]}\n",
    "\n",
    "async with MCPServerStdio(params=fetch_params, client_session_timeout_seconds=60) as server:\n",
    "    tools = await server.list_tools()\n",
    "    print(f\"Fetch server loaded with {len(tools)} tool(s)\")\n",
    "\n",
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a57b20",
   "metadata": {},
   "source": [
    "## Test 2: NPX-based MCP Server (Filesystem)\n",
    "\n",
    "Testing that npx-based MCP servers work with the patch wrapping them with `cmd /c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a332e47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sandbox_path = os.path.abspath(os.path.join(os.getcwd(), \"sandbox\"))\n",
    "os.makedirs(sandbox_path, exist_ok=True)\n",
    "\n",
    "fs_params = {\n",
    "    \"command\": \"npx\", \n",
    "    \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", sandbox_path]\n",
    "}\n",
    "\n",
    "async with MCPServerStdio(params=fs_params, client_session_timeout_seconds=60) as server:\n",
    "    tools = await server.list_tools()\n",
    "    print(f\"Filesystem server loaded with {len(tools)} tool(s)\")\n",
    "tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c47391d",
   "metadata": {},
   "source": [
    "## Test 3: Agent with Both Servers\n",
    "\n",
    "Create an agent that uses both uvx and npx MCP servers together.\n",
    "\n",
    "The agent will fetch content from a website and save it to a file using both servers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad101159",
   "metadata": {},
   "source": [
    "### Tracing Configuration\n",
    "\n",
    "When using Ollama, we configure a custom trace processor to capture execution traces locally. Unlike OpenAI's platform which provides built-in trace visualization at platform.openai.com/traces, Ollama requires a local tracing solution.\n",
    "\n",
    "This gives you full visibility into the agent's execution flow, tool calls, and LLM interactions when running locally with Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dee8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.tracing import set_trace_processors, add_trace_processor\n",
    "from custom_tracing_processor import CustomTraceProcessor\n",
    "\n",
    "if USE_OLLAMA:\n",
    "    set_trace_processors([CustomTraceProcessor()])\n",
    "else:\n",
    "    add_trace_processor(CustomTraceProcessor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a180abb",
   "metadata": {},
   "source": [
    "### Web fetch and write to filesystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5d8686",
   "metadata": {},
   "source": [
    "**Note**: The fetch tool may include extra content in responses even with the `raw` parameter. While `gpt-4o-mini` handles this better by filtering unwanted content, this doesn't affect the core demonstration of Windows MCP server compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_params = {\"command\": \"uvx\", \"args\": [\"mcp-server-fetch\"]}\n",
    "sandbox_path = os.path.abspath(os.path.join(os.getcwd(), \"sandbox\"))\n",
    "fs_params = {\n",
    "    \"command\": \"npx\", \n",
    "    \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", sandbox_path]\n",
    "}\n",
    "\n",
    "instructions = \"\"\"\n",
    "You are a specialized agent for downloading web content and saving it to files.\n",
    "\n",
    "You have access to two tools:\n",
    "1. fetch - downloads web content and returns it in the 'text' field. Always use the 'raw' parameter set to true.\n",
    "2. write_file - saves text content to a file using 'path' and 'content' parameters\n",
    "\n",
    "Always execute tools one at a time. After fetch returns, extract the text value and use it in write_file.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Download the webpage from https://example.com and save it to sandbox/demo_jpt.html\n",
    "\n",
    "Execute step by step:\n",
    "1. Use fetch to download from https://example.com\n",
    "2. After fetch succeeds, use write_file to save the downloaded text to sandbox/demo_jpt.html\n",
    "\"\"\"\n",
    "\n",
    "async with MCPServerStdio(params=fetch_params, client_session_timeout_seconds=60) as fetch_server:\n",
    "    async with MCPServerStdio(params=fs_params, client_session_timeout_seconds=60) as fs_server:\n",
    "\n",
    "        agent = Agent(\n",
    "            name=\"demo_agent\",\n",
    "            instructions=instructions,\n",
    "            model=MODEL,\n",
    "            mcp_servers=[fetch_server, fs_server]\n",
    "        )\n",
    "        \n",
    "        with trace(\"windows_no_wsl\"):\n",
    "            result = await Runner.run(\n",
    "                agent, \n",
    "                prompt\n",
    "            )\n",
    "        \n",
    "        print(f\"Agent completed task\")\n",
    "        print(f\"Result: {result.final_output}\")\n",
    "        if not USE_OLLAMA:\n",
    "            print(f\"\\nView trace at: https://platform.openai.com/traces\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
